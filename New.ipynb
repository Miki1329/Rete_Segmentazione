{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d0b2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mik\\.conda\\envs\\Pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import tensorboard\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import gc\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader ,TensorDataset\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import clear_output\n",
    "datapath=''\n",
    "train_data_path = '' \n",
    "test_data_path = ''\n",
    "image_depth=8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_image_paths = [] #to store image paths in list\n",
    "masks = [] #to store masks\n",
    "image_size = 352\n",
    "size= 352\n",
    "\n",
    "norm = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef99f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.img_dir =  os.path.join(datapath, 'Train_f/')\n",
    "        self.lab_dir = os.path.join(datapath, 'Mask_f/')\n",
    "        self.dirtra_lis= [f for f in listdir(self.img_dir) if isfile(join(self.img_dir, f))]\n",
    "        self.dirlab_lis= [f for f in listdir(self.img_dir) if isfile(join(self.lab_dir, f))]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dirtra_lis)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #self.img = torch.Tensor(np.load('imgs_train.npy'))\n",
    "        #self.img_labels = torch.Tensor(np.load('imgs_mask_train.npy'))\n",
    "        imgpath=os.path.join(self.img_dir,self.dirtra_lis[idx])\n",
    "        maskpath=os.path.join(self.lab_dir,self.dirlab_lis[idx])\n",
    "        image = torch.Tensor(np.load(imgpath))\n",
    "        label = torch.Tensor(np.load(maskpath))\n",
    "        #image = T.Resize(size=size)(image)\n",
    "        #label = T.Resize(size=size)(label)\n",
    "        image=T.CenterCrop(size=size)(image)\n",
    "        label=T.CenterCrop(size=size)(label)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfae540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValImageDataset(Dataset):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.test_dir =  os.path.join(datapath, 'Test_f/')\n",
    "        self.val_dir = os.path.join(datapath, 'Val_f/')\n",
    "        self.dirtest_lis= [f for f in listdir(self.test_dir) if isfile(join(self.test_dir, f))]\n",
    "        self.dirval_lis= [f for f in listdir(self.val_dir) if isfile(join(self.val_dir, f))]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dirtest_lis)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #self.val_labels = torch.Tensor(np.load('val_test.npy'))\n",
    "        #self.val = torch.Tensor(np.load('imgs_test.npy'))\n",
    "        testpath=os.path.join(self.test_dir,self.dirtest_lis[idx])\n",
    "        valpath=os.path.join(self.val_dir,self.dirval_lis[idx])\n",
    "        image = torch.Tensor(np.load(testpath))\n",
    "        label = torch.Tensor(np.load(valpath))\n",
    "        #image = T.Resize(size=size)(image)\n",
    "        #label = T.Resize(size=size)(label)\n",
    "        image=T.CenterCrop(size=size)(image)\n",
    "        label=T.CenterCrop(size=size)(label)\n",
    " \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de240ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504\n"
     ]
    }
   ],
   "source": [
    "imgs_train=TrainImageDataset()\n",
    "train_dataloader = DataLoader(imgs_train, batch_size=1, shuffle=True,num_workers=0,pin_memory=True)\n",
    "z=len(imgs_train)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7015a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "imgs_test=ValImageDataset()\n",
    "validation_loader = DataLoader(imgs_test, batch_size=1, shuffle=False,num_workers=0,pin_memory=True)\n",
    "print(len(imgs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8b937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_3d(in_dim, out_dim, activation):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        activation,)\n",
    "\n",
    "\n",
    "def conv_trans_block_3d(in_dim, out_dim, activation):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose3d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        activation,)\n",
    "\n",
    "\n",
    "def max_pooling_3d():\n",
    "    return nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "\n",
    "def conv_block_2_3d(in_dim, out_dim, activation):\n",
    "    return nn.Sequential(\n",
    "        conv_block_3d(in_dim, out_dim, activation),\n",
    "        nn.Conv3d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm3d(out_dim),)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_filters):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_filters = num_filters\n",
    "        activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        # Down sampling\n",
    "        self.down_1 = conv_block_2_3d(self.in_dim, self.num_filters, activation)\n",
    "        self.pool_1 = max_pooling_3d()\n",
    "        self.down_2 = conv_block_2_3d(self.num_filters, self.num_filters * 2, activation)\n",
    "        self.pool_2 = max_pooling_3d()\n",
    "        self.down_3 = conv_block_2_3d(self.num_filters * 2, self.num_filters * 4, activation)\n",
    "        self.pool_3 = max_pooling_3d()\n",
    "        #self.down_4 = conv_block_2_3d(self.num_filters * 4, self.num_filters * 8, activation)\n",
    "        #self.pool_4 = max_pooling_3d()\n",
    "        #self.down_5 = conv_block_2_3d(self.num_filters * 8, self.num_filters * 16, activation)\n",
    "        #self.pool_5 = max_pooling_3d()\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = conv_block_2_3d(self.num_filters * 4, self.num_filters * 8, activation)\n",
    "        \n",
    "        # Up sampling\n",
    "        #self.trans_1 = conv_trans_block_3d(self.num_filters * 32, self.num_filters * 32, activation)\n",
    "        #self.up_1 = conv_block_2_3d(self.num_filters * 48, self.num_filters * 16, activation)\n",
    "        #self.trans_2 = conv_trans_block_3d(self.num_filters * 8, self.num_filters * 16, activation)\n",
    "        #self.up_2 = conv_block_2_3d(self.num_filters * 24, self.num_filters * 8, activation)\n",
    "        self.trans_3 = conv_trans_block_3d(self.num_filters * 8, self.num_filters * 4, activation)\n",
    "        self.up_3 = conv_block_2_3d(self.num_filters * 8, self.num_filters * 4, activation)\n",
    "        self.trans_4 = conv_trans_block_3d(self.num_filters * 4, self.num_filters * 2, activation)\n",
    "        self.up_4 = conv_block_2_3d(self.num_filters * 4, self.num_filters * 2, activation)\n",
    "        self.trans_5 = conv_trans_block_3d(self.num_filters * 2, self.num_filters , activation)\n",
    "        self.up_5 = conv_block_2_3d(self.num_filters*2 , self.num_filters, activation)\n",
    "        \n",
    "        # Output\n",
    "        self.out = conv_block_3d(self.num_filters, out_dim, activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Down sampling\n",
    "        down_1 = self.down_1(x) # -> [1, 8, 8, 64, 64]\n",
    "        pool_1 = self.pool_1(down_1) # -> [1, 1, 4, , 64, 64]\n",
    "        \n",
    "        down_2 = self.down_2(pool_1) # -> [1,1, 8, 64, 64, 64]\n",
    "        pool_2 = self.pool_2(down_2) # -> [1, 8, 32, 32, 32]\n",
    "        \n",
    "        down_3 = self.down_3(pool_2) # -> [1, 16, 32, 32, 32]\n",
    "        pool_3 = self.pool_3(down_3) # -> [1, 16, 16, 16, 16]\n",
    "        \n",
    "        #down_4 = self.down_4(pool_3) # -> [1, 32, 16, 16, 16]\n",
    "        #pool_4 = self.pool_4(down_4) # -> [1, 32, 8, 8, 8]\n",
    "        \n",
    "\n",
    "        \n",
    "        # Bridge\n",
    "        bridge = self.bridge(pool_3) # -> [1, 128, 4, 4, 4]\n",
    "        \n",
    "        # Up sampling\n",
    "        \n",
    "        #trans_2 = self.trans_2(pool_4) # -> [1, 64, 16, 16, 16]\n",
    "        #concat_2 = torch.cat([trans_2, down_4], dim=1) # -> [1, 96, 16, 16, 16]\n",
    "        #up_2 = self.up_2(concat_2) # -> [1, 32, 16, 16, 16]\n",
    "        \n",
    "        trans_3 = self.trans_3(bridge) # -> [1, 32, 32, 32, 32]\n",
    "        concat_3 = torch.cat([trans_3, down_3], dim=1) # -> [1, 48, 32, 32, 32]\n",
    "        up_3 = self.up_3(concat_3) # -> [1, 16, 32, 32, 32]\n",
    "        \n",
    "        trans_4 = self.trans_4(up_3) # -> [1, 16, 64, 64, 64]\n",
    "        concat_4 = torch.cat([trans_4, down_2], dim=1) # -> [1, 24, 64, 64, 64]\n",
    "        up_4 = self.up_4(concat_4) # -> [1, 8, 64, 64, 64]\n",
    "        \n",
    "        trans_5 = self.trans_5(up_4) # -> [1, 8, 128, 128, 128]\n",
    "        concat_5 = torch.cat([trans_5, down_1], dim=1) # -> [1, 12, 128, 128, 128]\n",
    "        up_5 = self.up_5(concat_5) # -> [1, 4, 128, 128, 128]\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(up_5) # -> [1, 3, 128, 128, 128]\n",
    "        return out\n",
    "\n",
    "\n",
    "model = UNet(in_dim=1, out_dim=1, num_filters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3599a97f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displ=0\n",
    "model.to(device=device)\n",
    "if(displ==0):\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88eaea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth=1\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = torch.flatten(y_true)\n",
    "    y_pred_f = torch.flatten(y_pred)\n",
    "    intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return ((-dice_coef(y_true, y_pred))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d38a9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "opt='adam'\n",
    "if(opt=='sgd'):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "if(opt=='adam'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "#use_amp = True\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf84443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        start=time()\n",
    "        # Every data instance is an input + label pair\n",
    "        #with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.cuda()\n",
    "        labels=labels.cuda()\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        outputs=outputs.cuda()\n",
    "        # Compute the loss and its gradients\n",
    "        loss = dice_coef_loss(outputs, labels)\n",
    "        #scaler.scale(loss).backward()\n",
    "        #scaler.step(optimizer)\n",
    "        #scaler.update()\n",
    "        #optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += float(loss.item())\n",
    "        if i == (len(imgs_train)-1):\n",
    "            last_loss = running_loss / len(imgs_train) # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        end=time()\n",
    "        print(str(np.around(((end-start)*(z-i)/60),decimals=2))+'Min' + \" Sec Remaining \"+\" Loss = \"+str(float(loss.item())),end=\"                                   \\r\")\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda276c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 22 51 49\n",
      "EPOCH 1:\n",
      "47.47Min Sec Remaining  Loss = -6.759258667443646e-06                                     \r"
     ]
    }
   ],
   "source": [
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "pred_dir = \"Predictions\"\n",
    "imgs = np.ndarray((1,16, size, size), dtype=np.uint8)\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%H %M %S')\n",
    "writer = SummaryWriter('runs/UnetSeg_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "print(\"Started at \" + timestamp)\n",
    "\n",
    "best_vloss = 1\n",
    "gc.enable()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.cuda\n",
    "    model.train(True)\n",
    "    start1=time()\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    end1=time()\n",
    "    torch.cuda.empty_cache()\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        \n",
    "        startval=time()\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs=vinputs.cuda()\n",
    "        vlabels=vlabels.cuda()\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = dice_coef_loss(voutputs, vlabels)\n",
    "        running_vloss += float(vloss)\n",
    "        endval=time()\n",
    "        print(\"Val Time= \" + str(np.around((endval-startval),decimals=2)),end=\"                                                         \\r\")\n",
    "    avg_vloss = float (running_vloss / (i + 1))\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',{ 'Training' : avg_loss, 'Validation' : avg_vloss },epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state.\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_number += 1\n",
    "    print('Elsp_time ')\n",
    "    print((str(np.around((end1-start1)*(EPOCHS-epoch_number)/60,decimals=1))+\" Min \"))\n",
    "    gc.collect()\n",
    "timestamp1 = datetime.now().strftime('%H %M %S')\n",
    "print(\"End at \" + timestamp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79691286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del(vinputs)\n",
    "#del(vlabels)\n",
    "#del(inputs)\n",
    "#del(labels)\n",
    "#del(train_dataloader)\n",
    "#del(imgs_train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca36dd",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2d0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag=1\n",
    "if(flag==1):\n",
    "    imgs = np.ndarray((len(validation_loader),8, size, size), dtype=np.uint8)#Mask\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        #vinputs=vinputs.cuda()\n",
    "        voutputs = model(vinputs)\n",
    "        #print(voutputs.shape[0]+str(i)+ \"\\r\")\n",
    "        imgs[i,:,:,:]=voutputs.detach().numpy()\n",
    "    print(\"Pred Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85512712",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = \"Predictions\"\n",
    "count_processed=0\n",
    "ou_dir =os.path.join(pred_dir,'epoch' + str(epoch))\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.mkdir(pred_dir)\n",
    "for x in range(0, imgs.shape[0]):\n",
    "    for y in range(0, imgs.shape[1]):\n",
    "        imsave(os.path.join(pred_dir, 'predictions_' + str(x)+' slice '+str(y) + '.png'), (imgs[x][y])*255)\n",
    "        count_processed += 1\n",
    "print('Saving to .npy files done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f8f86",
   "metadata": {},
   "source": [
    "ou_dir =os.path.join(pred_dir,'epoch' + str(epoch))\n",
    "    if not os.path.exists(ou_dir):\n",
    "        os.mkdir(ou_dir)\n",
    "    for x in range(0, len(imgs_test)):\n",
    "        imgs=voutputs[0,x].to(norm)\n",
    "        imgs=imgs.detach().numpy()\n",
    "        for y in range(0,16):\n",
    "            imsave(os.path.join(ou_dir, 'pre_processed_' + str(y) + '.png'), ((imgs[0][y])*255))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd603ecf",
   "metadata": {},
   "source": [
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Conv3d, ConvTranspose3d, BatchNorm3d, MaxPool3d, AvgPool1d, Dropout3d\n",
    "from torch.nn import ReLU, Sigmoid\n",
    "import torch\n",
    "\n",
    "\n",
    "class UNet(Module):\n",
    "    # __                            __\n",
    "    #  1|__   ________________   __|1\n",
    "    #     2|__  ____________  __|2\n",
    "    #        3|__  ______  __|3\n",
    "    #           4|__ __ __|4\n",
    "    # The convolution operations on either side are residual subject to 1*1 Convolution for channel homogeneity\n",
    "\n",
    "    def __init__(self, num_channels=1, feat_channels=[8, 16, 32, 64, 128], residual='conv'):\n",
    "        # residual: conv for residual input x through 1*1 conv across every layer for downsampling, None for removal of residuals\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder downsamplers\n",
    "        self.pool1 = MaxPool3d((2, 2, 2))\n",
    "        self.pool2 = MaxPool3d((2, 2, 2))\n",
    "        self.pool3 = MaxPool3d((2, 2, 2))\n",
    "        self.pool4 = MaxPool3d((2, 2, 2))\n",
    "\n",
    "        # Encoder convolutions\n",
    "        self.conv_blk1 = Conv3D_Block(num_channels, feat_channels[0], residual=residual)\n",
    "        self.conv_blk2 = Conv3D_Block(feat_channels[0], feat_channels[1], residual=residual)\n",
    "        self.conv_blk3 = Conv3D_Block(feat_channels[1], feat_channels[2], residual=residual)\n",
    "        self.conv_blk4 = Conv3D_Block(feat_channels[2], feat_channels[3], residual=residual)\n",
    "        self.conv_blk5 = Conv3D_Block(feat_channels[3], feat_channels[4], residual=residual)\n",
    "\n",
    "        # Decoder convolutions\n",
    "        self.dec_conv_blk4 = Conv3D_Block(2 * feat_channels[3], feat_channels[3], residual=residual)\n",
    "        self.dec_conv_blk3 = Conv3D_Block(2 * feat_channels[2], feat_channels[2], residual=residual)\n",
    "        self.dec_conv_blk2 = Conv3D_Block(2 * feat_channels[1], feat_channels[1], residual=residual)\n",
    "        self.dec_conv_blk1 = Conv3D_Block(2 * feat_channels[0], feat_channels[0], residual=residual)\n",
    "\n",
    "        # Decoder upsamplers\n",
    "        self.deconv_blk4 = Deconv3D_Block(feat_channels[4], feat_channels[3])\n",
    "        self.deconv_blk3 = Deconv3D_Block(feat_channels[3], feat_channels[2])\n",
    "        self.deconv_blk2 = Deconv3D_Block(feat_channels[2], feat_channels[1])\n",
    "        self.deconv_blk1 = Deconv3D_Block(feat_channels[1], feat_channels[0])\n",
    "\n",
    "        # Final 1*1 Conv Segmentation map\n",
    "        self.one_conv = Conv3d(feat_channels[0], num_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Activation function\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "\n",
    "        x1 = self.conv_blk1(x)\n",
    "\n",
    "        x_low1 = self.pool1(x1)\n",
    "        x2 = self.conv_blk2(x_low1)\n",
    "\n",
    "        x_low2 = self.pool2(x2)\n",
    "        x3 = self.conv_blk3(x_low2)\n",
    "\n",
    "        x_low3 = self.pool3(x3)\n",
    "        x4 = self.conv_blk4(x_low3)\n",
    "\n",
    "        x_low4 = self.pool4(x4)\n",
    "        base = self.conv_blk5(x_low4)\n",
    "\n",
    "        # Decoder part\n",
    "\n",
    "        d4 = torch.cat([self.deconv_blk4(base), x4], dim=1)\n",
    "        d_high4 = self.dec_conv_blk4(d4)\n",
    "\n",
    "        d3 = torch.cat([self.deconv_blk3(d_high4), x3], dim=1)\n",
    "        d_high3 = self.dec_conv_blk3(d3)\n",
    "        d_high3 = Dropout3d(p=0.5)(d_high3)\n",
    "\n",
    "        d2 = torch.cat([self.deconv_blk2(d_high3), x2], dim=1)\n",
    "        d_high2 = self.dec_conv_blk2(d2)\n",
    "        d_high2 = Dropout3d(p=0.5)(d_high2)\n",
    "\n",
    "        d1 = torch.cat([self.deconv_blk1(d_high2), x1], dim=1)\n",
    "        d_high1 = self.dec_conv_blk1(d1)\n",
    "\n",
    "        seg = self.sigmoid(self.one_conv(d_high1))\n",
    "\n",
    "        return seg\n",
    "\n",
    "\n",
    "class Conv3D_Block(Module):\n",
    "\n",
    "    def __init__(self, inp_feat, out_feat, kernel=3, stride=1, padding=1, residual=None):\n",
    "\n",
    "        super(Conv3D_Block, self).__init__()\n",
    "\n",
    "        self.conv1 = Sequential(\n",
    "            Conv3d(inp_feat, out_feat, kernel_size=kernel,\n",
    "                   stride=stride, padding=padding, bias=True),\n",
    "            BatchNorm3d(out_feat),\n",
    "            ReLU())\n",
    "\n",
    "        self.conv2 = Sequential(\n",
    "            Conv3d(out_feat, out_feat, kernel_size=kernel,\n",
    "                   stride=stride, padding=padding, bias=True),\n",
    "            BatchNorm3d(out_feat),\n",
    "            ReLU())\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "        if self.residual is not None:\n",
    "            self.residual_upsampler = Conv3d(inp_feat, out_feat, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = x\n",
    "\n",
    "        if not self.residual:\n",
    "            return self.conv2(self.conv1(x))\n",
    "        else:\n",
    "            return self.conv2(self.conv1(x)) + self.residual_upsampler(res)\n",
    "\n",
    "\n",
    "class Deconv3D_Block(Module):\n",
    "\n",
    "    def __init__(self, inp_feat, out_feat, kernel=3, stride=2, padding=1):\n",
    "        super(Deconv3D_Block, self).__init__()\n",
    "\n",
    "        self.deconv = Sequential(\n",
    "            ConvTranspose3d(inp_feat, out_feat, kernel_size=(kernel, kernel, kernel),\n",
    "                            stride=(stride, stride, stride), padding=(padding, padding, padding), output_padding=1, bias=True),\n",
    "            ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.deconv(x)\n",
    "\n",
    "\n",
    "class ChannelPool3d(AvgPool1d):\n",
    "\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(ChannelPool3d, self).__init__(kernel_size, stride, padding)\n",
    "        self.pool_1d = AvgPool1d(self.kernel_size, self.stride, self.padding, self.ceil_mode)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        n, c, d, w, h = inp.size()\n",
    "        inp = inp.view(n, c, d * w * h).permute(0, 2, 1)\n",
    "        pooled = self.pool_1d(inp)\n",
    "        c = int(c / self.kernel_size[0])\n",
    "        return inp.view(n, c, d, w, h)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    model =UNet(residual='pool').cuda()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
